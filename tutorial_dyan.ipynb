{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Miniproject Tutorial Notebook\n",
    "\n",
    "This notebook is here to guide you through the basics of the frameworks necessary for you to do well on your CS456-Miniproject 🤓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium environments\n",
    "\n",
    "One of the main and most spread environment developer in the field of RL research is [Gymnasium](https://gymnasium.farama.org/). They provide standardized environments offering a large range of difficulties and setups, that are perfectly designed to benchmark performances of RL and Deep RL algorithms.\n",
    "\n",
    "The main structure is very simple to understand. First, we need to instantiate our environment. We will use an existing environment, but one could also use their structure to design their own environment.\n",
    "\n",
    "Let's directly work with the Mountain Car environment that will be used in the project. \n",
    "\n",
    "_PS: If you're more curious, feel free to browse the large list available on their website!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment contains an action space and an observation (state) space. Let's see what these look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions available: 3\n",
      "Observation shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of actions available: {env.action_space.n}\")\n",
    "print(f\"Observation shape: {env.observation_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the action space of that first environment is discrete and contains 3 possible actions: accelerate to the left, don't accelerate and accelerate to the right. \n",
    "\n",
    "The observation space has a dimension of 2, and you can find what each part represents [here](https://gymnasium.farama.org/environments/classic_control/mountain_car/#observation-space).\n",
    "\n",
    "Before taking actions, the environment should be reset (or boostrapped). **Note: this should be done every time the environment has to be restarted, i.e., at the end of any episode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: [-0.53400785  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# the second return value is an info dictionary, but it doesn't contain anything in this environment\n",
    "starting_state, _ = env.reset() \n",
    "\n",
    "print(f\"Starting state: {starting_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what the actions look like and that the environment is ready, we can take actions inside it. This is done using the `env.step` function, that takes an action as input, and returns multiple values. More details on each of them can be found [here](https://gymnasium.farama.org/api/env/#gymnasium.Env.step).\n",
    "\n",
    "In the project, you will have an agent that will choose an action (based on the policy learned) given the current state. However, for now, we can simply sample actions at random using `action_space.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 2\n",
      "Next state: [-0.5329298   0.00107806]\n",
      "Reward: -1.0\n",
      "Terminated: False\n",
      "Truncated: False\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(f\"Sampled action: {action}\")\n",
    "next_state, reward, terminated, truncated, _ = env.step(action) # again, the last return value is an empty info object\n",
    "\n",
    "print(f\"Next state: {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Terminated: {terminated}\")\n",
    "print(f\"Truncated: {truncated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `terminated` and `truncated`  variables represent the two ways that the episode might be done. Thus, it might be handy to use\n",
    "```\n",
    "done = terminated or truncated\n",
    "```\n",
    "in your code. 💡\n",
    "\n",
    "We now have all the pieces necessary to run a full episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward after taking random actions: -200.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terimnated, truncated, _ = env.step(action)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "    state = next_state\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"Episode reward after taking random actions: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your goal in the project will be to code an agent that can beat that 🙃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "class DynaAgent:\n",
    "    def __init__(self, env, discr_step=(0.025, 0.005), gamma=0.99, alpha=0.1, epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.995, k=10):\n",
    "        self.env = env\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.alpha = alpha  # 学习率\n",
    "        self.epsilon = epsilon  # 初始探索率\n",
    "        self.epsilon_min = epsilon_min  # 最小探索率\n",
    "        self.epsilon_decay = epsilon_decay  # 探索率衰减\n",
    "        self.k = k  # 每次更新Q值时进行的模拟更新次数\n",
    "        \n",
    "        self.discr_step = discr_step  # 离散化步长\n",
    "        # 计算离散状态空间的大小\n",
    "        self.num_states = (int((env.observation_space.high[0] - env.observation_space.low[0]) / discr_step[0]) + 1,\n",
    "                           int((env.observation_space.high[1] - env.observation_space.low[1]) / discr_step[1]) + 1)\n",
    "        self.num_actions = env.action_space.n  # 动作空间的大小\n",
    "        \n",
    "        # 初始化Q表格，以及状态-动作计数和状态-动作-下一状态计数\n",
    "        self.Q = np.zeros((*self.num_states, self.num_actions))  # Q值表格\n",
    "        self.state_action_counts = np.zeros((*self.num_states, self.num_actions))  # 状态-动作计数\n",
    "        self.state_action_next_state_counts = np.zeros((*self.num_states, self.num_actions, *self.num_states))  # 状态-动作-下一状态计数\n",
    "        self.R = np.zeros((*self.num_states, self.num_actions))  # 奖励表格\n",
    "\n",
    "        # 初始化转移概率矩阵 P\n",
    "        self.P = np.full((*self.num_states, self.num_actions, *self.num_states), 1.0 / np.prod(self.num_states))  # 初始化为均匀分布\n",
    "        \n",
    "        # 打印状态空间和动作空间的信息，用于调试\n",
    "        print(f\"State space size: {self.num_states}\")\n",
    "        print(f\"State space low: {env.observation_space.low}, high: {env.observation_space.high}\")\n",
    "        print(f\"Number of actions: {self.num_actions}\")\n",
    "\n",
    "    def discretize(self, state):\n",
    "        # 将连续状态空间离散化\n",
    "        low = self.env.observation_space.low\n",
    "        high = self.env.observation_space.high\n",
    "        # 分别计算两个维度的离散状态\n",
    "        discr_state = (\n",
    "            int((state[0] - low[0]) / self.discr_step[0]),\n",
    "            int((state[1] - low[1]) / self.discr_step[1])\n",
    "        )\n",
    "        # 使用np.clip确保离散状态在有效范围内\n",
    "        discr_state = np.clip(discr_state, (0, 0), (self.num_states[0] - 1, self.num_states[1] - 1))\n",
    "        return discr_state\n",
    "\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        # 将当前状态和下一状态离散化\n",
    "        discr_state = self.discretize(state)\n",
    "        discr_next_state = self.discretize(next_state)\n",
    "        \n",
    "        # 更新状态-动作计数和状态-动作-下一状态计数\n",
    "        self.state_action_counts[discr_state[0], discr_state[1], action] += 1\n",
    "        self.state_action_next_state_counts[discr_state[0], discr_state[1], action, discr_next_state[0], discr_next_state[1]] += 1\n",
    "        self.R[discr_state[0], discr_state[1], action] = ((self.R[discr_state[0], discr_state[1], action] * (self.state_action_counts[discr_state[0], discr_state[1], action] - 1)) + reward) / self.state_action_counts[discr_state[0], discr_state[1], action]\n",
    "\n",
    "        # 更新转移概率矩阵 P\n",
    "        total_next_state_counts = np.sum(self.state_action_next_state_counts[discr_state[0], discr_state[1], action])\n",
    "        if total_next_state_counts > 0:\n",
    "            self.P[discr_state[0], discr_state[1], action] = self.state_action_next_state_counts[discr_state[0], discr_state[1], action] / total_next_state_counts\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # 根据当前策略选择动作\n",
    "        discr_state = self.discretize(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # 以ε的概率选择随机动作（探索）\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            # 以1-ε的概率选择Q值最大的动作（利用）\n",
    "            return np.argmax(self.Q[discr_state[0], discr_state[1]])\n",
    "        \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # 使用实际转移更新Q值\n",
    "        discr_state = self.discretize(state)\n",
    "        discr_next_state = self.discretize(next_state)\n",
    "        \n",
    "        # 更新 Q 值\n",
    "        self.Q[discr_state[0], discr_state[1], action] += self.alpha * (\n",
    "            reward + self.gamma * np.max(self.Q[discr_next_state[0], discr_next_state[1]]) - \n",
    "            self.Q[discr_state[0], discr_state[1], action])\n",
    "\n",
    "        # 进行 k 次模拟更新\n",
    "        for _ in range(self.k):\n",
    "            rand_state = (np.random.randint(0, self.num_states[0]), np.random.randint(0, self.num_states[1]))\n",
    "            rand_action = np.random.randint(0, self.num_actions)\n",
    "            if self.state_action_counts[rand_state[0], rand_state[1], rand_action] == 0:\n",
    "                continue\n",
    "            next_state_prob = self.P[rand_state[0], rand_state[1], rand_action]\n",
    "            next_state_index = np.argmax(next_state_prob)\n",
    "            next_state = np.unravel_index(next_state_index, self.num_states)\n",
    "            reward = self.R[rand_state[0], rand_state[1], rand_action]\n",
    "            self.Q[rand_state[0], rand_state[1], rand_action] += self.alpha * (\n",
    "                reward + self.gamma * np.max(self.Q[next_state[0], next_state[1]]) - \n",
    "                self.Q[rand_state[0], rand_state[1], rand_action])\n",
    "        \n",
    "        # 衰减ε，逐渐减少探索率\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space size: (73, 29)\n",
      "State space low: [-1.2  -0.07], high: [0.6  0.07]\n",
      "Number of actions: 3\n",
      "Episode: 100, Total Reward: -458.0, Epsilon: 0.05\n",
      "Episode: 200, Total Reward: -407.0, Epsilon: 0.05\n",
      "Episode: 300, Total Reward: -347.0, Epsilon: 0.05\n",
      "Episode: 400, Total Reward: -382.0, Epsilon: 0.05\n",
      "Episode: 500, Total Reward: -320.0, Epsilon: 0.05\n",
      "Episode: 600, Total Reward: -253.0, Epsilon: 0.05\n",
      "Episode: 700, Total Reward: -340.0, Epsilon: 0.05\n",
      "Episode: 800, Total Reward: -260.0, Epsilon: 0.05\n",
      "Episode: 900, Total Reward: -206.0, Epsilon: 0.05\n",
      "Episode: 1000, Total Reward: -319.0, Epsilon: 0.05\n",
      "Episode: 1100, Total Reward: -233.0, Epsilon: 0.05\n",
      "Episode: 1200, Total Reward: -229.0, Epsilon: 0.05\n",
      "Episode: 1300, Total Reward: -190.0, Epsilon: 0.05\n",
      "Episode: 1400, Total Reward: -239.0, Epsilon: 0.05\n",
      "Episode: 1500, Total Reward: -167.0, Epsilon: 0.05\n",
      "Episode: 1600, Total Reward: -174.0, Epsilon: 0.05\n",
      "Episode: 1700, Total Reward: -172.0, Epsilon: 0.05\n",
      "Episode: 1800, Total Reward: -161.0, Epsilon: 0.05\n",
      "Episode: 1900, Total Reward: -231.0, Epsilon: 0.05\n",
      "Episode: 2000, Total Reward: -247.0, Epsilon: 0.05\n",
      "Episode: 2100, Total Reward: -273.0, Epsilon: 0.05\n",
      "Episode: 2200, Total Reward: -229.0, Epsilon: 0.05\n",
      "Episode: 2300, Total Reward: -160.0, Epsilon: 0.05\n",
      "Episode: 2400, Total Reward: -215.0, Epsilon: 0.05\n",
      "Episode: 2500, Total Reward: -158.0, Epsilon: 0.05\n",
      "Episode: 2600, Total Reward: -156.0, Epsilon: 0.05\n",
      "Episode: 2700, Total Reward: -159.0, Epsilon: 0.05\n",
      "Episode: 2800, Total Reward: -180.0, Epsilon: 0.05\n",
      "Episode: 2900, Total Reward: -175.0, Epsilon: 0.05\n",
      "Episode: 3000, Total Reward: -157.0, Epsilon: 0.05\n"
     ]
    }
   ],
   "source": [
    "# 创建Mountain Car环境\n",
    "env = gym.make('MountainCar-v0')\n",
    "# 实例化Dyna代理\n",
    "agent = DynaAgent(env)\n",
    "\n",
    "num_episodes = 3000  # 训练的episode数量\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()  # 重置环境\n",
    "    total_reward = 0\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.select_action(state)  # 选择动作\n",
    "        next_state, reward, done, _, _ = env.step(action)  # 执行动作，获取下一状态和奖励\n",
    "        \n",
    "        agent.observe(state, action, next_state, reward)  # 记录观察到的转移\n",
    "        agent.update(state, action, reward, next_state)  # 更新Q值\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制累计奖励\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Dyna Agent Training')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
