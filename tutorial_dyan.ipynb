{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Miniproject Tutorial Notebook\n",
    "\n",
    "This notebook is here to guide you through the basics of the frameworks necessary for you to do well on your CS456-Miniproject ğŸ¤“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium environments\n",
    "\n",
    "One of the main and most spread environment developer in the field of RL research is [Gymnasium](https://gymnasium.farama.org/). They provide standardized environments offering a large range of difficulties and setups, that are perfectly designed to benchmark performances of RL and Deep RL algorithms.\n",
    "\n",
    "The main structure is very simple to understand. First, we need to instantiate our environment. We will use an existing environment, but one could also use their structure to design their own environment.\n",
    "\n",
    "Let's directly work with the Mountain Car environment that will be used in the project. \n",
    "\n",
    "_PS: If you're more curious, feel free to browse the large list available on their website!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment contains an action space and an observation (state) space. Let's see what these look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(3)\n",
      "Observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions available: 3\n",
      "Observation shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of actions available: {env.action_space.n}\")\n",
    "print(f\"Observation shape: {env.observation_space.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the action space of that first environment is discrete and contains 3 possible actions: accelerate to the left, don't accelerate and accelerate to the right. \n",
    "\n",
    "The observation space has a dimension of 2, and you can find what each part represents [here](https://gymnasium.farama.org/environments/classic_control/mountain_car/#observation-space).\n",
    "\n",
    "Before taking actions, the environment should be reset (or boostrapped). **Note: this should be done every time the environment has to be restarted, i.e., at the end of any episode.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: [-0.53400785  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# the second return value is an info dictionary, but it doesn't contain anything in this environment\n",
    "starting_state, _ = env.reset() \n",
    "\n",
    "print(f\"Starting state: {starting_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what the actions look like and that the environment is ready, we can take actions inside it. This is done using the `env.step` function, that takes an action as input, and returns multiple values. More details on each of them can be found [here](https://gymnasium.farama.org/api/env/#gymnasium.Env.step).\n",
    "\n",
    "In the project, you will have an agent that will choose an action (based on the policy learned) given the current state. However, for now, we can simply sample actions at random using `action_space.sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled action: 2\n",
      "Next state: [-0.5329298   0.00107806]\n",
      "Reward: -1.0\n",
      "Terminated: False\n",
      "Truncated: False\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(f\"Sampled action: {action}\")\n",
    "next_state, reward, terminated, truncated, _ = env.step(action) # again, the last return value is an empty info object\n",
    "\n",
    "print(f\"Next state: {next_state}\")\n",
    "print(f\"Reward: {reward}\")\n",
    "print(f\"Terminated: {terminated}\")\n",
    "print(f\"Truncated: {truncated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `terminated` and `truncated`  variables represent the two ways that the episode might be done. Thus, it might be handy to use\n",
    "```\n",
    "done = terminated or truncated\n",
    "```\n",
    "in your code. ğŸ’¡\n",
    "\n",
    "We now have all the pieces necessary to run a full episode!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward after taking random actions: -200.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "state, _ = env.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terimnated, truncated, _ = env.step(action)\n",
    "\n",
    "    episode_reward += reward\n",
    "\n",
    "    state = next_state\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(f\"Episode reward after taking random actions: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your goal in the project will be to code an agent that can beat that ğŸ™ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "\n",
    "class DynaAgent:\n",
    "    def __init__(self, env, discr_step=(0.025, 0.005), gamma=0.99, alpha=0.1, epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.995, k=10):\n",
    "        self.env = env\n",
    "        self.gamma = gamma  # æŠ˜æ‰£å› å­\n",
    "        self.alpha = alpha  # å­¦ä¹ ç‡\n",
    "        self.epsilon = epsilon  # åˆå§‹æ¢ç´¢ç‡\n",
    "        self.epsilon_min = epsilon_min  # æœ€å°æ¢ç´¢ç‡\n",
    "        self.epsilon_decay = epsilon_decay  # æ¢ç´¢ç‡è¡°å‡\n",
    "        self.k = k  # æ¯æ¬¡æ›´æ–°Qå€¼æ—¶è¿›è¡Œçš„æ¨¡æ‹Ÿæ›´æ–°æ¬¡æ•°\n",
    "        \n",
    "        self.discr_step = discr_step  # ç¦»æ•£åŒ–æ­¥é•¿\n",
    "        # è®¡ç®—ç¦»æ•£çŠ¶æ€ç©ºé—´çš„å¤§å°\n",
    "        self.num_states = (int((env.observation_space.high[0] - env.observation_space.low[0]) / discr_step[0]) + 1,\n",
    "                           int((env.observation_space.high[1] - env.observation_space.low[1]) / discr_step[1]) + 1)\n",
    "        self.num_actions = env.action_space.n  # åŠ¨ä½œç©ºé—´çš„å¤§å°\n",
    "        \n",
    "        # åˆå§‹åŒ–Qè¡¨æ ¼ï¼Œä»¥åŠçŠ¶æ€-åŠ¨ä½œè®¡æ•°å’ŒçŠ¶æ€-åŠ¨ä½œ-ä¸‹ä¸€çŠ¶æ€è®¡æ•°\n",
    "        self.Q = np.zeros((*self.num_states, self.num_actions))  # Qå€¼è¡¨æ ¼\n",
    "        self.state_action_counts = np.zeros((*self.num_states, self.num_actions))  # çŠ¶æ€-åŠ¨ä½œè®¡æ•°\n",
    "        self.state_action_next_state_counts = np.zeros((*self.num_states, self.num_actions, *self.num_states))  # çŠ¶æ€-åŠ¨ä½œ-ä¸‹ä¸€çŠ¶æ€è®¡æ•°\n",
    "        self.R = np.zeros((*self.num_states, self.num_actions))  # å¥–åŠ±è¡¨æ ¼\n",
    "\n",
    "        # åˆå§‹åŒ–è½¬ç§»æ¦‚ç‡çŸ©é˜µ P\n",
    "        self.P = np.full((*self.num_states, self.num_actions, *self.num_states), 1.0 / np.prod(self.num_states))  # åˆå§‹åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ\n",
    "        \n",
    "        # æ‰“å°çŠ¶æ€ç©ºé—´å’ŒåŠ¨ä½œç©ºé—´çš„ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•\n",
    "        print(f\"State space size: {self.num_states}\")\n",
    "        print(f\"State space low: {env.observation_space.low}, high: {env.observation_space.high}\")\n",
    "        print(f\"Number of actions: {self.num_actions}\")\n",
    "\n",
    "    def discretize(self, state):\n",
    "        # å°†è¿ç»­çŠ¶æ€ç©ºé—´ç¦»æ•£åŒ–\n",
    "        low = self.env.observation_space.low\n",
    "        high = self.env.observation_space.high\n",
    "        # åˆ†åˆ«è®¡ç®—ä¸¤ä¸ªç»´åº¦çš„ç¦»æ•£çŠ¶æ€\n",
    "        discr_state = (\n",
    "            int((state[0] - low[0]) / self.discr_step[0]),\n",
    "            int((state[1] - low[1]) / self.discr_step[1])\n",
    "        )\n",
    "        # ä½¿ç”¨np.clipç¡®ä¿ç¦»æ•£çŠ¶æ€åœ¨æœ‰æ•ˆèŒƒå›´å†…\n",
    "        discr_state = np.clip(discr_state, (0, 0), (self.num_states[0] - 1, self.num_states[1] - 1))\n",
    "        return discr_state\n",
    "\n",
    "    def observe(self, state, action, next_state, reward):\n",
    "        # å°†å½“å‰çŠ¶æ€å’Œä¸‹ä¸€çŠ¶æ€ç¦»æ•£åŒ–\n",
    "        discr_state = self.discretize(state)\n",
    "        discr_next_state = self.discretize(next_state)\n",
    "        \n",
    "        # æ›´æ–°çŠ¶æ€-åŠ¨ä½œè®¡æ•°å’ŒçŠ¶æ€-åŠ¨ä½œ-ä¸‹ä¸€çŠ¶æ€è®¡æ•°\n",
    "        self.state_action_counts[discr_state[0], discr_state[1], action] += 1\n",
    "        self.state_action_next_state_counts[discr_state[0], discr_state[1], action, discr_next_state[0], discr_next_state[1]] += 1\n",
    "        self.R[discr_state[0], discr_state[1], action] = ((self.R[discr_state[0], discr_state[1], action] * (self.state_action_counts[discr_state[0], discr_state[1], action] - 1)) + reward) / self.state_action_counts[discr_state[0], discr_state[1], action]\n",
    "\n",
    "        # æ›´æ–°è½¬ç§»æ¦‚ç‡çŸ©é˜µ P\n",
    "        total_next_state_counts = np.sum(self.state_action_next_state_counts[discr_state[0], discr_state[1], action])\n",
    "        if total_next_state_counts > 0:\n",
    "            self.P[discr_state[0], discr_state[1], action] = self.state_action_next_state_counts[discr_state[0], discr_state[1], action] / total_next_state_counts\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # æ ¹æ®å½“å‰ç­–ç•¥é€‰æ‹©åŠ¨ä½œ\n",
    "        discr_state = self.discretize(state)\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # ä»¥Îµçš„æ¦‚ç‡é€‰æ‹©éšæœºåŠ¨ä½œï¼ˆæ¢ç´¢ï¼‰\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            # ä»¥1-Îµçš„æ¦‚ç‡é€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œï¼ˆåˆ©ç”¨ï¼‰\n",
    "            return np.argmax(self.Q[discr_state[0], discr_state[1]])\n",
    "        \n",
    "    def update(self, state, action, reward, next_state):\n",
    "        # ä½¿ç”¨å®é™…è½¬ç§»æ›´æ–°Qå€¼\n",
    "        discr_state = self.discretize(state)\n",
    "        discr_next_state = self.discretize(next_state)\n",
    "        \n",
    "        # æ›´æ–° Q å€¼\n",
    "        self.Q[discr_state[0], discr_state[1], action] += self.alpha * (\n",
    "            reward + self.gamma * np.max(self.Q[discr_next_state[0], discr_next_state[1]]) - \n",
    "            self.Q[discr_state[0], discr_state[1], action])\n",
    "\n",
    "        # è¿›è¡Œ k æ¬¡æ¨¡æ‹Ÿæ›´æ–°\n",
    "        for _ in range(self.k):\n",
    "            rand_state = (np.random.randint(0, self.num_states[0]), np.random.randint(0, self.num_states[1]))\n",
    "            rand_action = np.random.randint(0, self.num_actions)\n",
    "            if self.state_action_counts[rand_state[0], rand_state[1], rand_action] == 0:\n",
    "                continue\n",
    "            next_state_prob = self.P[rand_state[0], rand_state[1], rand_action]\n",
    "            next_state_index = np.argmax(next_state_prob)\n",
    "            next_state = np.unravel_index(next_state_index, self.num_states)\n",
    "            reward = self.R[rand_state[0], rand_state[1], rand_action]\n",
    "            self.Q[rand_state[0], rand_state[1], rand_action] += self.alpha * (\n",
    "                reward + self.gamma * np.max(self.Q[next_state[0], next_state[1]]) - \n",
    "                self.Q[rand_state[0], rand_state[1], rand_action])\n",
    "        \n",
    "        # è¡°å‡Îµï¼Œé€æ¸å‡å°‘æ¢ç´¢ç‡\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space size: (73, 29)\n",
      "State space low: [-1.2  -0.07], high: [0.6  0.07]\n",
      "Number of actions: 3\n",
      "Episode: 100, Total Reward: -458.0, Epsilon: 0.05\n",
      "Episode: 200, Total Reward: -407.0, Epsilon: 0.05\n",
      "Episode: 300, Total Reward: -347.0, Epsilon: 0.05\n",
      "Episode: 400, Total Reward: -382.0, Epsilon: 0.05\n",
      "Episode: 500, Total Reward: -320.0, Epsilon: 0.05\n",
      "Episode: 600, Total Reward: -253.0, Epsilon: 0.05\n",
      "Episode: 700, Total Reward: -340.0, Epsilon: 0.05\n",
      "Episode: 800, Total Reward: -260.0, Epsilon: 0.05\n",
      "Episode: 900, Total Reward: -206.0, Epsilon: 0.05\n",
      "Episode: 1000, Total Reward: -319.0, Epsilon: 0.05\n",
      "Episode: 1100, Total Reward: -233.0, Epsilon: 0.05\n",
      "Episode: 1200, Total Reward: -229.0, Epsilon: 0.05\n",
      "Episode: 1300, Total Reward: -190.0, Epsilon: 0.05\n",
      "Episode: 1400, Total Reward: -239.0, Epsilon: 0.05\n",
      "Episode: 1500, Total Reward: -167.0, Epsilon: 0.05\n",
      "Episode: 1600, Total Reward: -174.0, Epsilon: 0.05\n",
      "Episode: 1700, Total Reward: -172.0, Epsilon: 0.05\n",
      "Episode: 1800, Total Reward: -161.0, Epsilon: 0.05\n",
      "Episode: 1900, Total Reward: -231.0, Epsilon: 0.05\n",
      "Episode: 2000, Total Reward: -247.0, Epsilon: 0.05\n",
      "Episode: 2100, Total Reward: -273.0, Epsilon: 0.05\n",
      "Episode: 2200, Total Reward: -229.0, Epsilon: 0.05\n",
      "Episode: 2300, Total Reward: -160.0, Epsilon: 0.05\n",
      "Episode: 2400, Total Reward: -215.0, Epsilon: 0.05\n",
      "Episode: 2500, Total Reward: -158.0, Epsilon: 0.05\n",
      "Episode: 2600, Total Reward: -156.0, Epsilon: 0.05\n",
      "Episode: 2700, Total Reward: -159.0, Epsilon: 0.05\n",
      "Episode: 2800, Total Reward: -180.0, Epsilon: 0.05\n",
      "Episode: 2900, Total Reward: -175.0, Epsilon: 0.05\n",
      "Episode: 3000, Total Reward: -157.0, Epsilon: 0.05\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºMountain Carç¯å¢ƒ\n",
    "env = gym.make('MountainCar-v0')\n",
    "# å®ä¾‹åŒ–Dynaä»£ç†\n",
    "agent = DynaAgent(env)\n",
    "\n",
    "num_episodes = 3000  # è®­ç»ƒçš„episodeæ•°é‡\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()  # é‡ç½®ç¯å¢ƒ\n",
    "    total_reward = 0\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.select_action(state)  # é€‰æ‹©åŠ¨ä½œ\n",
    "        next_state, reward, done, _, _ = env.step(action)  # æ‰§è¡ŒåŠ¨ä½œï¼Œè·å–ä¸‹ä¸€çŠ¶æ€å’Œå¥–åŠ±\n",
    "        \n",
    "        agent.observe(state, action, next_state, reward)  # è®°å½•è§‚å¯Ÿåˆ°çš„è½¬ç§»\n",
    "        agent.update(state, action, reward, next_state)  # æ›´æ–°Qå€¼\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    rewards.append(total_reward)\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶ç´¯è®¡å¥–åŠ±\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Dyna Agent Training')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
